#pragma kernel ProcessRawDepthData

// Use Metal-friendly structured buffer instead of RWByteAddressBuffer
RWStructuredBuffer<uint> rawDepthData;
// Color texture (JPEG decoded once to GPU texture)
Texture2D<float4> colorTexture;
SamplerState samplercolorTexture;

// Output structured buffer
struct VertexData
{
    float3 vertex;
    float4 color;
    int isValid;
};
RWStructuredBuffer<VertexData> outputVertices;
RWStructuredBuffer<int> validCount;

// Camera metadata structure
struct CameraMetadata
{
    // Transform matrices
    float4x4 d2cRotation;
    float3 d2cTranslation;
    float4x4 depthViewerTransform;

    // Camera intrinsics
    float fx_d, fy_d, cx_d, cy_d; // Depth camera
    float fx_c, fy_c, cx_c, cy_c; // Color camera

    // Distortion parameters
    float k1_d, k2_d, k3_d, k4_d, k5_d, k6_d, p1_d, p2_d; // Depth distortion
    float k1_c, k2_c, k3_c, k4_c, k5_c, k6_c, p1_c, p2_c; // Color distortion

    // Image dimensions
    uint depthWidth;
    uint depthHeight;
    uint colorWidth;
    uint colorHeight;

    // Processing parameters
    float depthScaleFactor;
    float depthBias;
    int useOpenCVLUT;

    // Bounding volume parameters
    int hasBoundingVolume;
    int showAllPoints;
    float4x4 boundingVolumeInverseTransform;

    // Buffer offsets (for multi-camera, not used in single camera)
    uint depthDataOffset;
    uint lutDataOffset;
    uint outputOffset;
};

StructuredBuffer<CameraMetadata> cameraMetadata;
RWStructuredBuffer<float2> depthUndistortLUT;

// No longer need metadata/offset parameters - data is pre-processed on CPU

float2 DistortColorProjection(float x_norm, float y_norm, CameraMetadata meta)
{
    float k1 = meta.k1_c, k2 = meta.k2_c, k3 = meta.k3_c;
    float k4 = meta.k4_c, k5 = meta.k5_c, k6 = meta.k6_c;
    float p1 = meta.p1_c, p2 = meta.p2_c;

    float r2 = x_norm * x_norm + y_norm * y_norm;
    float r4 = r2 * r2;
    float r6 = r4 * r2;

    float radial = 1 + k1 * r2 + k2 * r4 + k3 * r6;
    float x_d = x_norm * radial + 2 * p1 * x_norm * y_norm + p2 * (r2 + 2 * x_norm * x_norm);
    float y_d = y_norm * radial + 2 * p2 * x_norm * y_norm + p1 * (r2 + 2 * y_norm * y_norm);

    float u = meta.fx_c * x_d + meta.cx_c;
    float v = meta.fy_c * y_d + meta.cy_c;

    return float2(u, v);
}

bool IsPointInBoundingVolume(float3 worldPoint, CameraMetadata meta)
{
    // Early return if no bounding volume is set
    if (meta.hasBoundingVolume == 0) return true;

    float4 localPoint4 = mul(meta.boundingVolumeInverseTransform, float4(worldPoint, 1.0));
    float3 localPoint = localPoint4.xyz;

    return (abs(localPoint.x) <= 0.5) && (abs(localPoint.y) <= 0.5) && (abs(localPoint.z) <= 0.5);
}

// No longer need byte-level access function - use structured buffer directly

[numthreads(64,1,1)]
void ProcessRawDepthData(uint3 id : SV_DispatchThreadID)
{
    CameraMetadata meta = cameraMetadata[0];

    uint i = id.x;
    uint totalPixels = meta.depthWidth * meta.depthHeight;

    if (i >= totalPixels) return;

    int x = i % meta.depthWidth;
    int y = i / meta.depthWidth;

    // Read depth value directly from structured buffer (metadata already skipped)
    uint rawDepthValue = rawDepthData[i] & 0xFFFF; // Extract 16-bit depth value

    // Apply depth bias correction and scale factor
    float correctedDepth = rawDepthValue + meta.depthBias;
    float z = correctedDepth * (meta.depthScaleFactor / 1000.0);
    if (z <= 0)
    {
        outputVertices[i].isValid = 0;
        return;
    }

    float px, py;

    if (meta.useOpenCVLUT == 1)
    {
        // Method 1: OpenCV-generated undistortion LUT
        float2 rayCoords = depthUndistortLUT[x + y * meta.depthWidth];
        px = rayCoords.x * z;
        py = rayCoords.y * z;
    }
    else
    {
        // Method 2: Simple pinhole camera model
        px = (x - meta.cx_d) * z / meta.fx_d;
        py = (y - meta.cy_d) * z / meta.fy_d;
    }

    float3 dPoint = float3(px, py, z);
    float3 cPoint = mul(meta.d2cRotation, float4(dPoint, 1.0)).xyz + meta.d2cTranslation;

    // Skip points behind camera
    if (cPoint.z <= 0)
    {
        outputVertices[i].isValid = 0;
        return;
    }

    float x_norm = cPoint.x / cPoint.z;
    float y_norm = cPoint.y / cPoint.z;
    float2 colorPixelCoord = DistortColorProjection(x_norm, y_norm, meta);

    // Convert to normalized texture coordinates [0,1]
    float2 colorUV = float2(
        colorPixelCoord.x / (float)meta.colorWidth,
        1.0 - (colorPixelCoord.y / (float)meta.colorHeight) // Flip Y
    );

    // Sample color from GPU texture (no CPU conversion needed!)
    float4 color = colorTexture.SampleLevel(samplercolorTexture, colorUV, 0);

    // Check if color is valid (not completely black)
    bool hasValidColor = true;// color.r > 0.02 || color.g > 0.02 || color.b > 0.02; // Slight threshold for float comparison

    // Convert cPoint to world coordinates for bounding volume check
    float4 worldPoint4 = mul(meta.depthViewerTransform, float4(cPoint, 1.0));
    float3 worldPoint = worldPoint4.xyz;

    // Only add points with valid colors and within bounding volume
    bool withinBounds = (meta.showAllPoints == 1);
    if (meta.showAllPoints == 0) {
        withinBounds = IsPointInBoundingVolume(worldPoint, meta);
    }

    if (hasValidColor && withinBounds)
    {
        outputVertices[i].vertex = cPoint;
        outputVertices[i].color = color;
        outputVertices[i].isValid = 1;
        InterlockedAdd(validCount[0], 1);
    }
    else
    {
        outputVertices[i].isValid = 0;
    }
}