#pragma kernel ProcessRawDepthData

// Use Metal-friendly structured buffer instead of RWByteAddressBuffer
RWStructuredBuffer<uint> rawDepthData;
// Color texture (JPEG decoded once to GPU texture)
Texture2D<float4> colorTexture;
SamplerState samplercolorTexture;

// Output structured buffer
struct VertexData
{
    float3 vertex;
    float4 color;
    int isValid;
};
RWStructuredBuffer<VertexData> outputVertices;
RWStructuredBuffer<int> validCount;

// Camera and transform parameters
float4x4 rotationMatrix;
float3 translation;
float depthScaleFactor;
float depthBias;

// Depth camera intrinsics
float fx_d, fy_d, cx_d, cy_d;
// Color camera intrinsics  
float fx_c, fy_c, cx_c, cy_c;

// Distortion parameters
float4 colorDistortion; // k1, k2, p1, p2
float4 colorDistortion2; // k3, k4, k5, k6

// Image dimensions
int depthWidth;
int depthHeight;
int colorWidth;
int colorHeight;

// Processing options
bool useOpenCVLUT;
RWStructuredBuffer<float2> depthUndistortLUT;

// Bounding volume for culling
bool hasBoundingVolume;
bool showAllPoints;
float4x4 boundingVolumeInverseTransform;
float4x4 depthViewerTransform;

// No longer need metadata/offset parameters - data is pre-processed on CPU

float2 DistortColorProjection(float x_norm, float y_norm)
{
    float k1 = colorDistortion.x, k2 = colorDistortion.y, k3 = colorDistortion2.x;
    float k4 = colorDistortion2.y, k5 = colorDistortion2.z, k6 = colorDistortion2.w;
    float p1 = colorDistortion.z, p2 = colorDistortion.w;

    float r2 = x_norm * x_norm + y_norm * y_norm;
    float r4 = r2 * r2;
    float r6 = r4 * r2;

    float radial = 1 + k1 * r2 + k2 * r4 + k3 * r6;
    float x_d = x_norm * radial + 2 * p1 * x_norm * y_norm + p2 * (r2 + 2 * x_norm * x_norm);
    float y_d = y_norm * radial + 2 * p2 * x_norm * y_norm + p1 * (r2 + 2 * y_norm * y_norm);

    float u = fx_c * x_d + cx_c;
    float v = fy_c * y_d + cy_c;

    return float2(u, v);
}

bool IsPointInBoundingVolume(float3 worldPoint)
{
    if (!hasBoundingVolume) return true;
    
    float4 localPoint4 = mul(boundingVolumeInverseTransform, float4(worldPoint, 1.0));
    float3 localPoint = localPoint4.xyz;
    
    return abs(localPoint.x) <= 0.5 && abs(localPoint.y) <= 0.5 && abs(localPoint.z) <= 0.5;
}

// No longer need byte-level access function - use structured buffer directly

[numthreads(64,1,1)]
void ProcessRawDepthData(uint3 id : SV_DispatchThreadID)
{
    uint i = id.x;
    uint totalPixels = depthWidth * depthHeight;
    
    if (i >= totalPixels) return;
    
    int x = i % depthWidth;
    int y = i / depthWidth;
    
    // Read depth value directly from structured buffer (metadata already skipped)
    uint rawDepthValue = rawDepthData[i] & 0xFFFF; // Extract 16-bit depth value
    
    // Apply depth bias correction and scale factor
    float correctedDepth = rawDepthValue + depthBias;
    float z = correctedDepth * (depthScaleFactor / 1000.0);
    if (z <= 0) 
    {
        outputVertices[i].isValid = 0;
        return;
    }
    
    float px, py;
    
    if (useOpenCVLUT)
    {
        // Method 1: OpenCV-generated undistortion LUT
        float2 rayCoords = depthUndistortLUT[x + y * depthWidth];
        px = rayCoords.x * z;
        py = rayCoords.y * z;
    }
    else
    {
        // Method 2: Simple pinhole camera model
        px = (x - cx_d) * z / fx_d;
        py = (y - cy_d) * z / fy_d;
    }
    
    float3 dPoint = float3(px, py, z);
    float3 cPoint = mul(rotationMatrix, dPoint) + translation;
    
    // Skip points behind camera
    if (cPoint.z <= 0)
    {
        outputVertices[i].isValid = 0;
        return;
    }
    
    float x_norm = cPoint.x / cPoint.z;
    float y_norm = cPoint.y / cPoint.z;
    float2 colorPixelCoord = DistortColorProjection(x_norm, y_norm);
    
    // Convert to normalized texture coordinates [0,1]
    float2 colorUV = float2(
        colorPixelCoord.x / (float)colorWidth,
        1.0 - (colorPixelCoord.y / (float)colorHeight) // Flip Y
    );
    
    // Sample color from GPU texture (no CPU conversion needed!)
    float4 color = colorTexture.SampleLevel(samplercolorTexture, colorUV, 0);
    
    // Check if color is valid (not completely black)
    bool hasValidColor = true;// color.r > 0.02 || color.g > 0.02 || color.b > 0.02; // Slight threshold for float comparison
    
    // Convert cPoint to world coordinates for bounding volume check
    float4 worldPoint4 = mul(depthViewerTransform, float4(cPoint, 1.0));
    float3 worldPoint = worldPoint4.xyz;
    
    // Only add points with valid colors and within bounding volume
    bool withinBounds = showAllPoints || IsPointInBoundingVolume(worldPoint);
    
    if (hasValidColor && withinBounds)
    {
        outputVertices[i].vertex = cPoint;
        outputVertices[i].color = color;
        outputVertices[i].isValid = 1;
        InterlockedAdd(validCount[0], 1);
    }
    else
    {
        outputVertices[i].isValid = 0;
    }
}